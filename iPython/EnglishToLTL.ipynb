{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "queries = [\n",
    "    'Did my average speed exceed 2 m/s?', #F(average > 5)\n",
    "    'Did I bike greater than 100 feet this week?', #F(tot_dist > 100)\n",
    "    'Did I stay above an altitude of 50 meters?', #G(attitude > 50)\n",
    "    'Did my altitude ever exceed 200 foot?', #F(attitude > 200)\n",
    "    'Was I always faster than 10 m/s?', #G(speed > 10)\n",
    "    'Did my speed exceed 5 m/s and my altitude always stayed above 50 m?', # F(speed > 5) && G(altitude > 50)\n",
    "    'Did I bike more than 250 meters and go faster than 5 m/s on average?', # F(average > 5) && F(distance > 50)\n",
    "    'Did I go slower than 5 m/s or bike more than 251 m?', # F(speed < 5) || F(distance > 251)\n",
    "    'Did I maintain a speed of greater than 5 m/s over 1 minute?', # FG_(0,60) (speed > 5)\n",
    "    'Did I keep an altitude of less than 20 meters for 1 hour?' # FG_(0,3600) (altitude > 20)\n",
    "#     'After biking 500 meters, did I exceed speed of 15 m/s?'\n",
    "#     'Did I exceed speed of 15 m/s after biking 500 meters?'\n",
    "]\n",
    "\n",
    "\n",
    "triggerPhrases = [\n",
    "    ['did i', 'do i'],\n",
    "    ['how many', 'how much']\n",
    "]\n",
    "# Should contain all //recognizable// words. \n",
    "synonyms = [\n",
    "    ['maintain','keep'], #G_(t1,t2)\n",
    "    ['stay','always'], #G\n",
    "    ['did','ever'], #F\n",
    "    ['greater','exceed','more','faster','above'], # >\n",
    "    ['less','below','slower'], # <\n",
    "    ['after'],\n",
    "    ['until'],\n",
    "    ['speed', 'velocity'],\n",
    "    ['distance'],\n",
    "    ['incline', 'hill'],\n",
    "    ['altitude', 'elevation', 'height'],\n",
    "    ['degree', 'º', '°', 'deg'],\n",
    "    ['meters per second','m/s','ms','mph', 'miles per hour'],\n",
    "    ['minute'],\n",
    "    ['second'],\n",
    "    ['hour'],\n",
    "    ['meter','m'],\n",
    "    ['foot','feet'],\n",
    "    ['of'],\n",
    "    ['average','mean'],\n",
    "    ['bike','travel'],\n",
    "    ['and','both'],\n",
    "    ['or']\n",
    "]\n",
    "unitWords = [  # Omit s if can be plural\n",
    "    'degree', 'mph', 'second', 'minute', 'hour', 'meter', 'foot', 'meters per second'\n",
    "]\n",
    "modifierWords = [  # Words that modify units\n",
    "    'average','altitude','bike'\n",
    "]\n",
    "connectingWords = [\n",
    "    'of','and','or'\n",
    "]\n",
    "operatorWords = [ # Order matters in this array\n",
    "    'after', 'until', 'greater', 'less', 'maintain', 'stay','did'\n",
    "]\n",
    "variableWords = [\n",
    "    'speed', 'altitude', 'distance', 'incline'\n",
    "]\n",
    "distanceUnits = [  # First unit is default supported\n",
    "    'meter', 'foot'\n",
    "]\n",
    "timeUnits = [  \n",
    "    'second', 'minute', 'hour'\n",
    "]\n",
    "speedUnits = [  \n",
    "    'meters per second', 'mph'\n",
    "]\n",
    "\n",
    "operatorForms = {\n",
    "    'maintain': [\n",
    "        ['maintain', 'BOOL_EXPR', 'VALUE']\n",
    "        ###['maintain', 'BOOL_EXPR', 'until']\n",
    "    ],\n",
    "    'after': [\n",
    "        ['BOOL_EXPR', 'after', 'BOOL_EXPR']\n",
    "    ],\n",
    "    #'until': []\n",
    "    'greater': [\n",
    "        ['REAL_EXPR', 'greater', 'REAL_EXPR']\n",
    "    ],\n",
    "    'less': [\n",
    "        ['REAL_EXPR', 'less', 'REAL_EXPR']\n",
    "    ]\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Create synonymDict mapping all synonyms to their canonical word\n",
    "synonymDict = {}\n",
    "for group in synonyms:\n",
    "    standardWord = group[0]\n",
    "    for word in group:\n",
    "        synonymDict[word] = standardWord\n",
    "\n",
    "all = string.maketrans('','')\n",
    "noDigs = all.translate(all, string.digits)\n",
    "noLetters = all.translate(all, string.letters)\n",
    "allowedLetters = ' abcdefghijklmnopqrstuvwxyz0123456789%.º°'\n",
    "notAllowed = all.translate(all, allowedLetters)\n",
    "def cleanQuery(query):\n",
    "    return query.lower().translate(all, notAllowed)\n",
    "\n",
    "\n",
    "# Attempts to return the canonical words in your query\n",
    "def wordToStandard(word):\n",
    "    if word is None:\n",
    "        return None\n",
    "    word = word.lower()  # Lower case\n",
    "    if len(word) == 0:\n",
    "        return None\n",
    "    if word[0] in '-0123456789':\n",
    "        #TODO: Maybe more processing of numbers\n",
    "        return word\n",
    "    if word in synonymDict:\n",
    "        return synonymDict[word]\n",
    "    word = word.translate(all, noLetters)  # Keep only letters\n",
    "    if word in synonymDict:\n",
    "        return synonymDict[word]\n",
    "    if word[-1] == 's':\n",
    "        word = word[:-1]\n",
    "        if word in synonymDict:\n",
    "            return synonymDict[word]\n",
    "    if word[-2:] == 'ed':\n",
    "        word = word[:-2]\n",
    "        if word in synonymDict:\n",
    "            return synonymDict[word]\n",
    "    return None\n",
    "\n",
    "\n",
    "class Token:\n",
    "    '''A word associated with its meaning'''\n",
    "    # Token types\n",
    "    UNKNOWN = 0\n",
    "    NONE = 1\n",
    "    OPERATOR = 2\n",
    "    CONNECTOR = 3\n",
    "    NUMBER = 4\n",
    "    UNIT = 5\n",
    "    VARIABLE = 6\n",
    "    MODIFIER = 7\n",
    "    def __init__(self, word, left=None, right=None):\n",
    "        self.origWord = word\n",
    "        self.word = wordToStandard(word)\n",
    "        self.left = wordToStandard(left)\n",
    "        self.right = wordToStandard(right)\n",
    "        if self.word in operatorWords:\n",
    "            self.type = Token.OPERATOR\n",
    "        elif self.word in modifierWords:\n",
    "            self.type = Token.MODIFIER\n",
    "        elif self.word in variableWords:\n",
    "            self.type = Token.VARIABLE\n",
    "        elif self.word in unitWords:\n",
    "            self.type = Token.UNIT\n",
    "        elif self.word in connectingWords:\n",
    "            self.type = Token.CONNECTOR\n",
    "        elif len(self.word) > 0 and self.word[0] in '-0123456789':\n",
    "            self.type = Token.NUMBER\n",
    "        else:\n",
    "            self.type = Token.NONE\n",
    "            if self.word is not None:\n",
    "                print repr(self.origWord), repr(self.word), 'not understood.'\n",
    "    def __str__(self):\n",
    "        typeStr = ['UNKNOWN', 'NONE', 'OPERATOR', 'CONNECTOR',\n",
    "                   'NUMBER', 'UNIT', 'VARIABLE','MODIFIER'][self.type]\n",
    "        return '<%s, %s>' % (self.word, typeStr)\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "# This gives you the recognized canonical words\n",
    "def splitQuery(query):\n",
    "    query = cleanQuery(query)\n",
    "    l = query.split()\n",
    "    tokens = []\n",
    "    for word in l:\n",
    "        standard = wordToStandard(word)\n",
    "        if standard is not None:\n",
    "            tokens.append(standard)\n",
    "    return tokens\n",
    "\n",
    "# Determine if query has a F or G operator and returns them if they exist\n",
    "def determineFGOperator(query):\n",
    "    tokens = tokenizeQuery(query)\n",
    "    splitToks = splitTokensByType(tokens)\n",
    "    if 'OPERATOR' in splitToks:\n",
    "        if 'did' in splitToks['OPERATOR']:\n",
    "            return 'did'\n",
    "        elif 'stay' in splitToks['OPERATOR']:\n",
    "            return 'stay'\n",
    "        else:\n",
    "            return ''\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "# Splits a query based on OR and AND, assuming only one of the two is being used\n",
    "def divideQueries(queries):\n",
    "    splitAnd = queries.split(' and ')\n",
    "    splitOr = queries.split(' or ')\n",
    "    andLength = len(splitAnd)\n",
    "    orLength = len(splitOr)\n",
    "    if andLength > orLength:\n",
    "        return (andLength,'AND',splitAnd)\n",
    "    elif orLength > andLength:\n",
    "        return (orLength,'OR',splitOr)\n",
    "    elif (orLength == 1 and andLength == 1):\n",
    "        return (1,'',[queries])\n",
    "    else:\n",
    "        print 'Improperly formatted string'\n",
    "        return ''  \n",
    "\n",
    "def queriesToSTL(queries):\n",
    "    result = divideQueries(queries)\n",
    "    numQueries = result[0]\n",
    "    operator = result[1]\n",
    "    extraTok = ''\n",
    "    for i in xrange(numQueries):\n",
    "        token_word = determineFGOperator(result[2][i])\n",
    "        if len(token_word) > 0 and not extraTok: # F or G operator found\n",
    "            extraTok = token_word\n",
    "            for j in xrange(1,numQueries):\n",
    "                result[2][j] = token_word + ' ' + result[2][j]\n",
    "    STL = '' \n",
    "    for i in xrange(numQueries):\n",
    "        partialSTL = queryToSTL(result[2][i])\n",
    "        if i > 0:\n",
    "            STL = (operator,partialSTL,STL)\n",
    "        else:\n",
    "            STL = partialSTL       \n",
    "    return STL\n",
    "    \n",
    "# This will tell you what type each splitted query is\n",
    "def tokenizeQuery(query):\n",
    "    split = splitQuery(query)\n",
    "    tokens = []\n",
    "    for i in xrange(len(split)):\n",
    "        left = None if i-1 < 0 else split[i-1]\n",
    "        right = None if i+1 >= len(split) else split[i+1]\n",
    "        t = Token(split[i], left, right)\n",
    "        if t.type != Token.UNKNOWN and t.type != Token.NONE:\n",
    "            if t.type == Token.NUMBER:\n",
    "                groupT = TokenGroup(TokenGroup.VALUE,[t, Token(split[i+1], left, right)])\n",
    "                tokens.append(groupT)\n",
    "            elif t.type == Token.UNIT:\n",
    "                continue \n",
    "            else:\n",
    "                tokens.append(t)\n",
    "    return tokens\n",
    "\n",
    "# This will take an array of tokens and then output a dictionary index by Token Types\n",
    "def splitTokensByType(tokens):\n",
    "    # Token types, copied from Token definition\n",
    "    UNKNOWN = 0\n",
    "    NONE = 1\n",
    "    OPERATOR = 2\n",
    "    CONNECTOR = 3\n",
    "    NUMBER = 4\n",
    "    UNIT = 5\n",
    "    VARIABLE = 6\n",
    "    MODIFIER = 7\n",
    "\n",
    "    splitToks = {}\n",
    "    #Separate out tokens by type\n",
    "    for token in tokens:\n",
    "        if token.type == OPERATOR:\n",
    "            if 'OPERATOR' in splitToks and (token.word not in splitToks['OPERATOR']):\n",
    "                splitToks['OPERATOR'].append(token.word)\n",
    "            else:\n",
    "                splitToks['OPERATOR'] = [token.word]\n",
    "        elif token.type == TokenGroup.VALUE:\n",
    "            if 'VALUE' in splitToks and (token.word not in splitToks['VALUE']):\n",
    "                splitToks['VALUE'].append(token.word)\n",
    "            else:\n",
    "                splitToks['VALUE'] = [token.word]\n",
    "        elif token.type == VARIABLE:\n",
    "            splitToks['VARIABLE'] = token.word\n",
    "        elif token.type == MODIFIER:\n",
    "            if 'MODIFIER' in splitToks:\n",
    "                splitToks['MODIFIER'].append(token.word)\n",
    "            else:\n",
    "                splitToks['MODIFIER'] = [token.word]\n",
    "        else:\n",
    "            pass\n",
    "    return splitToks\n",
    "\n",
    "def parseValue(val):\n",
    "    parts = val.split(' ',1)\n",
    "    num = parts[0]\n",
    "    unit = parts[1]\n",
    "    return num,unit\n",
    "    \n",
    "def queryToSTL(query):\n",
    "    #Assume only one variable per query        \n",
    "    tokens = tokenizeQuery(query)\n",
    "    splitToks = splitTokensByType(tokens)\n",
    "    tLim = determineInterval(splitToks)\n",
    "    numOperators = len(splitToks['OPERATOR'])\n",
    "    for i in xrange(numOperators):        \n",
    "        min_power = 100\n",
    "        min_operator = ''\n",
    "        variable,qty = determineVariable(splitToks)\n",
    "        #Choose the most bad ass operator\n",
    "        for j in xrange(len(splitToks['OPERATOR'])):\n",
    "            operator = splitToks['OPERATOR'][j]\n",
    "            power = operatorWords.index(operator)\n",
    "            if power < min_power:\n",
    "                min_power = power\n",
    "                min_operator = operator\n",
    "        #Apply the bad ass operator\n",
    "#         print 'var:  ',variable\n",
    "#         print 'min_op:  ',min_operator\n",
    "        if variable == '':\n",
    "            if min_operator == 'did':\n",
    "                STL = ('F',STL)\n",
    "            elif min_operator == 'stay':\n",
    "                STL = ('G',STL)\n",
    "            elif min_operator == 'maintain':\n",
    "                STL = ('G_(0,%s)'%tLim,STL)\n",
    "        else:\n",
    "            if min_operator == 'greater':\n",
    "                STL = ('>',variable,qty)\n",
    "            elif min_operator == 'less':\n",
    "                STL = ('<',variable,qty)       \n",
    "        rem_tokens = []\n",
    "        if variable != '':            \n",
    "            for token in tokens:\n",
    "                if token.word in variable.split():\n",
    "                    pass\n",
    "                elif token.word in splitToks['VALUE']:\n",
    "                    pass\n",
    "                elif token.word == min_operator:\n",
    "                    pass\n",
    "                else:\n",
    "                    rem_tokens.append(token)\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                if token.word == min_operator:\n",
    "                    pass\n",
    "                else:\n",
    "                    rem_tokens.append(token)\n",
    "        splitToks = splitTokensByType(rem_tokens)\n",
    "        tokens = rem_tokens\n",
    "    return STL\n",
    "\n",
    "    \n",
    "# Determine if query has time interval information\n",
    "def determineInterval(splitToks):    \n",
    "    if 'VALUE' in splitToks:\n",
    "        for val in splitToks['VALUE']:\n",
    "            num, unit = parseValue(val)\n",
    "            if unit == timeUnits[0]:\n",
    "                return num\n",
    "            else:\n",
    "                continue\n",
    "        return 'tmax'\n",
    "    else:\n",
    "        return 'tmax'\n",
    "    \n",
    "# Determine from tokens which variable you are interested in and how much of it\n",
    "def determineVariable(splitToks):\n",
    "    variable = '',''\n",
    "    if 'VALUE' in splitToks:\n",
    "        for val in splitToks['VALUE']:\n",
    "            num,unit = parseValue(val)\n",
    "            if 'meters per second' == unit:\n",
    "                if 'MODIFIER' in splitToks:\n",
    "                    if splitToks['MODIFIER'] == ['average']:\n",
    "                        variable = 'average speed',num\n",
    "                else:\n",
    "                    variable = 'speed',num\n",
    "            elif 'meter'== unit:\n",
    "                variable = 'distance',num\n",
    "                if 'MODIFIER' in splitToks:\n",
    "                    if splitToks['MODIFIER'] == ['altitude']:\n",
    "                        variable = 'altitude',num\n",
    "            if variable != ('',''):\n",
    "                break\n",
    "    return variable\n",
    "\n",
    "\n",
    "class TokenGroup:\n",
    "    '''A group of tokens that mean somthing as a group'''\n",
    "    # Group types\n",
    "    UNKNOWN = 100\n",
    "    VALUE = 101  # Number with unit\n",
    "    BOOL_EXPR = 102  # Logical or temporal\n",
    "    REAL_EXPR = 103\n",
    "    tokenPatterns = {\n",
    "        VALUE: [\n",
    "            [Token.NUMBER, Token.UNIT],\n",
    "            [Token.NUMBER]\n",
    "        ],\n",
    "        BOOL_EXPR: [\n",
    "            # Equality operator\n",
    "            [Token.VARIABLE, Token.CONNECTOR, VALUE],\n",
    "            [VALUE, Token.VARIABLE],\n",
    "            []\n",
    "        ]\n",
    "    }\n",
    "    def __init__(self, type, tokens):\n",
    "        self.type = type\n",
    "        self.tokens = tokens\n",
    "        if tokens[1].origWord not in [distanceUnits[0], speedUnits[0], timeUnits[0]]:\n",
    "            notSupportedUnit = tokens[1].origWord\n",
    "            if notSupportedUnit in speedUnits:\n",
    "                number = 'unsupported unit'\n",
    "                unit = speedUnits[0]\n",
    "                if notSupportedUnit == 'mph':\n",
    "                    number = str(float(tokens[0].word)*0.44704)                                    \n",
    "            elif notSupportedUnit in timeUnits:\n",
    "                number = 'unsupported unit'\n",
    "                unit = timeUnits[0]\n",
    "                if notSupportedUnit == 'minute':\n",
    "                    number = str(float(tokens[0].word)*60)\n",
    "                if notSupportedUnit == 'hour':                    \n",
    "                    number = str(float(tokens[0].word)*60*60)\n",
    "            elif notSupportedUnit in distanceUnits:\n",
    "                number = 'unsupported unit'\n",
    "                unit = distanceUnits[0]\n",
    "                if notSupportedUnit == 'foot':\n",
    "                    number = str(float(tokens[0].word)*0.3048)                \n",
    "            self.word = '%s %s'%(number,unit) \n",
    "        else:\n",
    "            self.word = '%s %s'%(self.tokens[0].origWord,self.tokens[1].origWord) \n",
    "    def __str__(self):\n",
    "        typeStr = ['UNKNOWN', 'VAL', 'EXPRESSION'][self.type-100]\n",
    "        if self.type == TokenGroup.UNKNOWN:\n",
    "            return '<TokenGroup UNKNOWN>'\n",
    "        if self.type == TokenGroup.VALUE:\n",
    "            return '<%s, VALUE>'%self.word \n",
    "        if self.type == TokenGroup.BOOL_EXPR:\n",
    "            return '<EXPR>'\n",
    "        if self.type == TokenGroup.REAL_EXPR:\n",
    "            return '<EXPR>'\n",
    "            #return '<%s, %s>' % (repr(self.word), typeStr)\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did my average speed exceed 0.01 m/s?\n",
      "('F', ('>', 'average speed', '0.01'))\n",
      "\n",
      "\n",
      "Did I bike greater than 500 feet this week?\n",
      "('F', ('>', 'distance', '152.4'))\n",
      "\n",
      "\n",
      "Did I stay above an altitude of 50 meters?\n",
      "('F', ('G', ('>', 'altitude', '50')))\n",
      "\n",
      "\n",
      "Did my altitude ever exceed 10 foot?\n",
      "('F', ('>', 'altitude', '3.048'))\n",
      "\n",
      "\n",
      "Was I always faster than 10 m/s?\n",
      "('G', ('>', 'speed', '10'))\n",
      "\n",
      "\n",
      "Did my speed exceed 5 m/s and my altitude always stayed above 50 m?\n",
      "('AND', ('G', ('>', 'altitude', '50')), ('F', ('>', 'speed', '5')))\n",
      "\n",
      "\n",
      "Did I bike more than 250 meters and go faster than 5 m/s on average?\n",
      "('AND', ('F', ('>', 'average speed', '5')), ('F', ('>', 'distance', '250')))\n",
      "\n",
      "\n",
      "Did I go slower than 5 m/s or bike more than 251 m?\n",
      "('OR', ('F', ('>', 'distance', '251')), ('F', ('<', 'speed', '5')))\n",
      "\n",
      "\n",
      "Did I maintain a speed of greater than 5 m/s over 1 minute?\n",
      "('F', ('G_(0,60.0)', ('>', 'speed', '5')))\n",
      "\n",
      "\n",
      "Did I keep an altitude of less than 50 meters for 1 hour?\n",
      "('F', ('G_(0,3600.0)', ('<', 'altitude', '50')))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ind in xrange(len(queries)):\n",
    "    print queries[ind]\n",
    "    tokens = tokenizeQuery(queries[ind])\n",
    "    splitToks = splitTokensByType(tokens)\n",
    "    print queriesToSTL(queries[ind])\n",
    "    print\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('F', ('G_(0,3600.0)', ('<', 'altitude', '50')))"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'Did I keep an altitude of less than 50 meters for 1 hour?'\n",
    "# question = 'Did I keep a speed of greater than 50 mph for 2 minutes?'\n",
    "# question = 'Did I keep an altitude of greater than 50 meters for 2 minute?'\n",
    "tokens = tokenizeQuery(question)\n",
    "# splitToks = splitTokensByType(tokens)\n",
    "# determineVariable(splitToks)\n",
    "queriesToSTL(question)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
